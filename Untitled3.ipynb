{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNSKioYQlh3ZTErrV9ADBtg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install langchain==0.1.14 langchain-community==0.0.31 transformers==4.38.1 sentence-transformers==2.2.2 faiss-cpu==1.7.4 PyPDF2==3.0.1 python-docx==1.1.0 streamlit==1.32.0 google-generativeai==0.6.0 --upgrade --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "12JxrnZujPh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/hackathon_data.zip -d /content/"
      ],
      "metadata": {
        "id": "77ubVMc1qSYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os\n",
        "\n",
        "docs = []\n",
        "data_path = '/content/hackathon_data'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"Error: Data folder not found at {data_path}. Make sure you uploaded and unzipped 'hackathon_data.zip'.\")\n",
        "else:\n",
        "    for file_name in os.listdir(data_path):\n",
        "        file_path = os.path.join(data_path, file_name)\n",
        "        if file_name.endswith(\".txt\"):\n",
        "            loader = TextLoader(file_path)\n",
        "        elif file_name.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file_name.endswith(\".docx\"):\n",
        "            loader = Docx2txtLoader(file_path)\n",
        "        else:\n",
        "            continue\n",
        "        print(f\"Loading {file_name}...\")\n",
        "        try:\n",
        "            docs.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = text_splitter.split_documents(docs)\n",
        "    print(f\"Loaded {len(docs)} documents and split into {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "id": "IYsknBhqqV-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "print(\"Loading embedding model (this may take a moment)...\")\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "print(\"Embedding model loaded.\")\n",
        "\n",
        "print(\"Creating FAISS vector store...\")\n",
        "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "print(\"FAISS vector store created.\")"
      ],
      "metadata": {
        "id": "j_tJaB0VqaYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub --upgrade --force-reinstall --no-cache-dir\n",
        "!pip install sentence-transformers --upgrade --force-reinstall --no-cache-dir"
      ],
      "metadata": {
        "id": "nv534rQWp3A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Kipi New Joiner Companion (Basic Demo)\")\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "if prompt := st.chat_input(\"Ask me anything about Kipi onboarding...\"):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        response = f\"You asked: {prompt}. (AI response coming soon!)\" # Placeholder\n",
        "        st.markdown(response)\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n"
      ],
      "metadata": {
        "id": "4zhbqCL9qjRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "_cJA_2sgqoVS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}